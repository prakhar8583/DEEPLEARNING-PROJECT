{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prakhar8583/DEEPLEARNING-PROJECT/blob/main/automatic_ShakespeareLikebookwrittermodel_project_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qc-gBk4HFgtx"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QeZHCE_F4qg"
      },
      "outputs": [],
      "source": [
        "# lets get the file from the internet\n",
        "file=tf.keras.utils.get_file(\n",
        "    'shakespeare.text',\n",
        "    'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8eDCAabGzZn",
        "outputId": "54cf69fd-1a1d-4fe7-f086-2c5295de9793"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "type(file) # That the data type --- that is the strings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "guAYVd2RIGhj",
        "outputId": "02d377d6-8cc3-4d98-cf42-4795baa4c019"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/root/.keras/datasets/shakespeare.text'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXJrncZwIOUr"
      },
      "outputs": [],
      "source": [
        "# Lets read the characters from this file\n",
        "text=open(file,'rb').read().decode(encoding='utf8') # so file is read by the open command  and , { .read().decode(meanstion the incoding type like here utf8)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhD1wfHuJVba",
        "outputId": "c21cdfc1-213b-4610-a2d9-ab9f9ed7439b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ],
      "source": [
        "# after reading It store in text variable  , that is the string data type\n",
        "type(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmbAlkNqIn1z",
        "outputId": "e541fd1a-7812-48a8-f03f-75d3431a85c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# lets Print The Text , lets say First 100 characters\n",
        "print(text[:1000]) # IT Is printing the first 10000 character from the text data set or the file which I am download that"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvQyTa1qKLyz"
      },
      "source": [
        "*  ***futher we see the total lenth of the text in term of number of the character is 1115394 , that we going to process here : each and evey character is serving as componet in a squence  , for example In printed text F is first character then I is the second character , then R and S then T , then space Is another Character , Capital C Is another characters and so on , so it is good to know total number os the unique character in whole text-- for that we convert that into set to avoid the dublicate  to find that the total unique character***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YhWXo50I2D_",
        "outputId": "67520c56-aa5e-4797-fa9e-9aa1f7d95ad8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1115394"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ],
      "source": [
        "len(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yR7AH2eGKAzn",
        "outputId": "9f92257e-bea3-46c4-8082-3fe1bab77b26"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "65"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(set(text))# that 65 unique character form the vocabluary for that text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9wHbxjGMiOc"
      },
      "outputs": [],
      "source": [
        "vocab=sorted(set(text)) # Create the vocab  as just sort the assending order and form the vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlv9Zmq_NRat",
        "outputId": "dcc8720b-e67d-46ab-dd1b-e530f62e1fe5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['\\n',\n",
              " ' ',\n",
              " '!',\n",
              " '$',\n",
              " '&',\n",
              " \"'\",\n",
              " ',',\n",
              " '-',\n",
              " '.',\n",
              " '3',\n",
              " ':',\n",
              " ';',\n",
              " '?',\n",
              " 'A',\n",
              " 'B',\n",
              " 'C',\n",
              " 'D',\n",
              " 'E',\n",
              " 'F',\n",
              " 'G',\n",
              " 'H',\n",
              " 'I',\n",
              " 'J',\n",
              " 'K',\n",
              " 'L',\n",
              " 'M',\n",
              " 'N',\n",
              " 'O',\n",
              " 'P',\n",
              " 'Q',\n",
              " 'R',\n",
              " 'S',\n",
              " 'T',\n",
              " 'U',\n",
              " 'V',\n",
              " 'W',\n",
              " 'X',\n",
              " 'Y',\n",
              " 'Z',\n",
              " 'a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'd',\n",
              " 'e',\n",
              " 'f',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 'j',\n",
              " 'k',\n",
              " 'l',\n",
              " 'm',\n",
              " 'n',\n",
              " 'o',\n",
              " 'p',\n",
              " 'q',\n",
              " 'r',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'v',\n",
              " 'w',\n",
              " 'x',\n",
              " 'y',\n",
              " 'z']"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab # vocabulary is here is list of the chracter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gEzWhftjej0"
      },
      "source": [
        "so Next we need the array or dictnoary to map the index value to these character"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CN17G0J5NTVZ"
      },
      "outputs": [],
      "source": [
        "# here I am creating a dictionary of Indices and character using the enumwerate methord\n",
        "c2i={a:i for i,a in enumerate(vocab)} # c2i ----- means character to indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CyBfhihkGZH",
        "outputId": "6682f299-9fd1-4393-95fd-b589cc9a5c4c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'\\n': 0,\n",
              " ' ': 1,\n",
              " '!': 2,\n",
              " '$': 3,\n",
              " '&': 4,\n",
              " \"'\": 5,\n",
              " ',': 6,\n",
              " '-': 7,\n",
              " '.': 8,\n",
              " '3': 9,\n",
              " ':': 10,\n",
              " ';': 11,\n",
              " '?': 12,\n",
              " 'A': 13,\n",
              " 'B': 14,\n",
              " 'C': 15,\n",
              " 'D': 16,\n",
              " 'E': 17,\n",
              " 'F': 18,\n",
              " 'G': 19,\n",
              " 'H': 20,\n",
              " 'I': 21,\n",
              " 'J': 22,\n",
              " 'K': 23,\n",
              " 'L': 24,\n",
              " 'M': 25,\n",
              " 'N': 26,\n",
              " 'O': 27,\n",
              " 'P': 28,\n",
              " 'Q': 29,\n",
              " 'R': 30,\n",
              " 'S': 31,\n",
              " 'T': 32,\n",
              " 'U': 33,\n",
              " 'V': 34,\n",
              " 'W': 35,\n",
              " 'X': 36,\n",
              " 'Y': 37,\n",
              " 'Z': 38,\n",
              " 'a': 39,\n",
              " 'b': 40,\n",
              " 'c': 41,\n",
              " 'd': 42,\n",
              " 'e': 43,\n",
              " 'f': 44,\n",
              " 'g': 45,\n",
              " 'h': 46,\n",
              " 'i': 47,\n",
              " 'j': 48,\n",
              " 'k': 49,\n",
              " 'l': 50,\n",
              " 'm': 51,\n",
              " 'n': 52,\n",
              " 'o': 53,\n",
              " 'p': 54,\n",
              " 'q': 55,\n",
              " 'r': 56,\n",
              " 's': 57,\n",
              " 't': 58,\n",
              " 'u': 59,\n",
              " 'v': 60,\n",
              " 'w': 61,\n",
              " 'x': 62,\n",
              " 'y': 63,\n",
              " 'z': 64}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "c2i # as you can see here each character is assign with  a  number"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdWbGP_4kPwZ"
      },
      "outputs": [],
      "source": [
        "# We also Want I2C{ which is index to character array}\n",
        "i2c=np.array(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "nJwcAJpmkzp8",
        "outputId": "1d3c2427-db8e-47bf-a55d-673b0ade1d32"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "np.str_('x')"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "i2c[62]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAR_ijKTk7SE"
      },
      "source": [
        "*  Now We have two mapping__\n",
        "   * character to index mapping {c2i} #That is Is done by dictionary\n",
        "   * index to character mapping{i2C} # that is done by numpy array\n",
        "\n",
        "\n",
        "\n",
        "***next What we do we basically convert all our text in squence of integer rather then squence of character***\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E84-gJvIk3ai"
      },
      "outputs": [],
      "source": [
        "text_as_seq_of_ints=np.array([c2i[a] for a in text]) # that is bascially a list expenstion , so time know as list comprihencen\n",
        "# what happen is it actually pick the every character in the text, then it actually map character to index using this array c2i then return this text seq of integer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCSoNBymnCtE",
        "outputId": "8124aa0c-f2b7-4f35-dc4c-89c73cc8ac74"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([18, 47, 56, 57, 58,  1, 15, 47, 58, 47])"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_as_seq_of_ints[:10]# lets just print 10 character"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZ16l_oBnMuq",
        "outputId": "28dc2e7b-aed5-43f6-9842-fbd6af932afd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['F', 'i', 'r', 's', 't', ' ', 'C', 'i', 't', 'i'], dtype='<U1')"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# by the way If you want to print the coresponding text we going to use i2c mapping\n",
        "i2c[text_as_seq_of_ints[:10]] # as we see that each index is only one character"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjJwyYMfzRRI"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1ZbEPdCu9sW"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gh6OqLo8nkXb"
      },
      "outputs": [],
      "source": [
        "# lets define the length of the squence (means Inut or the coresponding target)\n",
        "seq_length=120# you acan select any arbitary number\n",
        "# then how many example per epoch\n",
        "examples_per_epoch=len(text)//(seq_length+1)# As We See that the eaxpamle or input per epoch is total length of data divided by seq_lenght +1\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruDqhQf1mQgv"
      },
      "source": [
        "now we are ready to convert our data set into yensor flow slices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Ojg1kULmLJG"
      },
      "outputs": [],
      "source": [
        "char_dataset=tf.data.Dataset.from_tensor_slices(text_as_seq_of_ints)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8ei-fm1mqzh",
        "outputId": "c1d10e1e-1e66-4c66-d83a-0630bb2cb466"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F 18\n",
            "i 47\n",
            "r 56\n",
            "s 57\n",
            "t 58\n",
            "  1\n",
            "C 15\n",
            "i 47\n",
            "t 58\n",
            "i 47\n",
            "z 64\n",
            "e 43\n",
            "n 52\n",
            ": 10\n",
            "\n",
            " 0\n",
            "B 14\n",
            "e 43\n",
            "f 44\n",
            "o 53\n",
            "r 56\n"
          ]
        }
      ],
      "source": [
        "# let Take 5 0r 10 slices or character in this particular sequence to display\n",
        "for i in char_dataset.take(20):\n",
        "  print(i2c[i.numpy()],i.numpy())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z45J_No6rC5_"
      },
      "source": [
        "next we need to convert dataset to the sequences that we really need , the sequences, like in the example Hello Word {lets make the squences first and then we will actually pick each sequences and covert the squences into the pair of inputs and output}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSJWki4XnTX7"
      },
      "outputs": [],
      "source": [
        "# lets make the squences first using the batch methord and size of the seq is (seq_length+1) and drop reminder in true\n",
        "seq=char_dataset.batch(seq_length+1,drop_remainder=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkZoup6snmM8",
        "outputId": "1667361f-aabb-43fa-e7e9-3b4cf5598297"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['F' 'i' 'r' 's' 't' ' ' 'C' 'i' 't' 'i' 'z' 'e' 'n' ':' '\\n' 'B' 'e' 'f'\n",
            " 'o' 'r' 'e' ' ' 'w' 'e' ' ' 'p' 'r' 'o' 'c' 'e' 'e' 'd' ' ' 'a' 'n' 'y'\n",
            " ' ' 'f' 'u' 'r' 't' 'h' 'e' 'r' ',' ' ' 'h' 'e' 'a' 'r' ' ' 'm' 'e' ' '\n",
            " 's' 'p' 'e' 'a' 'k' '.' '\\n' '\\n' 'A' 'l' 'l' ':' '\\n' 'S' 'p' 'e' 'a'\n",
            " 'k' ',' ' ' 's' 'p' 'e' 'a' 'k' '.' '\\n' '\\n' 'F' 'i' 'r' 's' 't' ' ' 'C'\n",
            " 'i' 't' 'i' 'z' 'e' 'n' ':' '\\n' 'Y' 'o' 'u' ' ' 'a' 'r' 'e' ' ' 'a' 'l'\n",
            " 'l' ' ' 'r' 'e' 's' 'o' 'l' 'v' 'e' 'd' ' ' 'r' 'a' 't']\n",
            "['h' 'e' 'r' ' ' 't' 'o' ' ' 'd' 'i' 'e' ' ' 't' 'h' 'a' 'n' ' ' 't' 'o'\n",
            " ' ' 'f' 'a' 'm' 'i' 's' 'h' '?' '\\n' '\\n' 'A' 'l' 'l' ':' '\\n' 'R' 'e'\n",
            " 's' 'o' 'l' 'v' 'e' 'd' '.' ' ' 'r' 'e' 's' 'o' 'l' 'v' 'e' 'd' '.' '\\n'\n",
            " '\\n' 'F' 'i' 'r' 's' 't' ' ' 'C' 'i' 't' 'i' 'z' 'e' 'n' ':' '\\n' 'F' 'i'\n",
            " 'r' 's' 't' ',' ' ' 'y' 'o' 'u' ' ' 'k' 'n' 'o' 'w' ' ' 'C' 'a' 'i' 'u'\n",
            " 's' ' ' 'M' 'a' 'r' 'c' 'i' 'u' 's' ' ' 'i' 's' ' ' 'c' 'h' 'i' 'e' 'f'\n",
            " ' ' 'e' 'n' 'e' 'm' 'y' ' ' 't' 'o' ' ' 't' 'h' 'e' ' ']\n"
          ]
        }
      ],
      "source": [
        "# let me print the sequences here\n",
        "for i in seq.take(2):\n",
        "  print(i2c[i.numpy()])# we print only two squences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AdowUwiZnyit"
      },
      "outputs": [],
      "source": [
        "# Each sequence actually contains the final character that we really want to predict by the give of rest of the sequence\n",
        "# so Now we need actually convert this each sequences into input and the corresponding target pair\n",
        "\n",
        "def f_make_input_target_pairs(s):\n",
        "  input_text=s[:-1]# what ever it recive start from the very begining and go to end but do not include the last charater\n",
        "\n",
        "  target_text=s[1:]# start from 1 and go to end , here we already know arrays start from 0\n",
        "  # this is basically right shifting , we not starting from the 0 we are starting from the one and going up to end\n",
        "  return input_text,target_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbJd0WByoP4y"
      },
      "outputs": [],
      "source": [
        "# craete the data set , we have map() function  in that map fuction we give data { what actually happen here actually map fuction take every sequence  and the data set will be generated }\n",
        "dataset=seq.map(f_make_input_target_pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqBEvr4AoYjz",
        "outputId": "61efa577-6abf-41a7-98eb-e11d08e126f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['F' 'i' 'r' 's' 't' ' ' 'C' 'i' 't' 'i' 'z' 'e' 'n' ':' '\\n' 'B' 'e' 'f'\n",
            " 'o' 'r' 'e' ' ' 'w' 'e' ' ' 'p' 'r' 'o' 'c' 'e' 'e' 'd' ' ' 'a' 'n' 'y'\n",
            " ' ' 'f' 'u' 'r' 't' 'h' 'e' 'r' ',' ' ' 'h' 'e' 'a' 'r' ' ' 'm' 'e' ' '\n",
            " 's' 'p' 'e' 'a' 'k' '.' '\\n' '\\n' 'A' 'l' 'l' ':' '\\n' 'S' 'p' 'e' 'a'\n",
            " 'k' ',' ' ' 's' 'p' 'e' 'a' 'k' '.' '\\n' '\\n' 'F' 'i' 'r' 's' 't' ' ' 'C'\n",
            " 'i' 't' 'i' 'z' 'e' 'n' ':' '\\n' 'Y' 'o' 'u' ' ' 'a' 'r' 'e' ' ' 'a' 'l'\n",
            " 'l' ' ' 'r' 'e' 's' 'o' 'l' 'v' 'e' 'd' ' ' 'r' 'a']\n",
            "['i' 'r' 's' 't' ' ' 'C' 'i' 't' 'i' 'z' 'e' 'n' ':' '\\n' 'B' 'e' 'f' 'o'\n",
            " 'r' 'e' ' ' 'w' 'e' ' ' 'p' 'r' 'o' 'c' 'e' 'e' 'd' ' ' 'a' 'n' 'y' ' '\n",
            " 'f' 'u' 'r' 't' 'h' 'e' 'r' ',' ' ' 'h' 'e' 'a' 'r' ' ' 'm' 'e' ' ' 's'\n",
            " 'p' 'e' 'a' 'k' '.' '\\n' '\\n' 'A' 'l' 'l' ':' '\\n' 'S' 'p' 'e' 'a' 'k'\n",
            " ',' ' ' 's' 'p' 'e' 'a' 'k' '.' '\\n' '\\n' 'F' 'i' 'r' 's' 't' ' ' 'C' 'i'\n",
            " 't' 'i' 'z' 'e' 'n' ':' '\\n' 'Y' 'o' 'u' ' ' 'a' 'r' 'e' ' ' 'a' 'l' 'l'\n",
            " ' ' 'r' 'e' 's' 'o' 'l' 'v' 'e' 'd' ' ' 'r' 'a' 't']\n"
          ]
        }
      ],
      "source": [
        "# lets take this data set firts pair of these data set\n",
        "# let just take first pair\n",
        "for X,y in dataset.take(1):\n",
        "  print(i2c[X.numpy()])\n",
        "  print(i2c[y.numpy()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1QVoB1ivYyp"
      },
      "source": [
        "*** LETS DEFINE THE SPECIFICS FOR THE MODEL AND LETS MAKE THE DATA SET BY SUFFLING IT TO REDUCE THE BIAS OF ANY TEMPORAL OR SEQUENTIAL ORDER***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDlwVoe5oq-t"
      },
      "outputs": [],
      "source": [
        "# LETS DEfine the batch size\n",
        "batch_size=50\n",
        "# Then We need to suffile the data set\n",
        "# Ensure dataset is only shuffled and batched once\n",
        "dataset=dataset.shuffle(1000).batch(batch_size,drop_remainder=True)\n",
        "\n",
        "\n",
        "\n",
        "# LETS SEE: we suffle the dataset ( in that we need to set the buffer size we set as the 1000 ) after suffling we really need to make the batches of the data  with batch_size\n",
        "\n",
        "\n",
        "# *** This particular command will basically prepare the dataset and dbias the data set by the sffuling the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mr0GBSkMwz5r"
      },
      "outputs": [],
      "source": [
        "# NEXT WE SET THE VOCB SIZE {VS}\n",
        "VS=len(vocab)# length of the vocab size\n",
        "ED=100# ED Is The Embedding dimension , so we want each character to be represented as 100 dimensional , dense vector\n",
        "# NU IS the number of the unit is gru or lstm or whatever\n",
        "NU=1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKoQ80VfxNC0"
      },
      "outputs": [],
      "source": [
        "# AFTER THIS WE REALLY READY TO DEFINE OUR MODEL\n",
        "def f_make_model(VS,ED,NU,batch_size):\n",
        "  model=tf.keras.Sequential([\n",
        "      tf.keras.Input(batch_shape=(batch_size, None)),\n",
        "      tf.keras.layers.Embedding(VS,ED),\n",
        "      tf.keras.layers.GRU(NU,return_sequences=True,stateful=True), # Corrected typo: tf.kreas to tf.keras\n",
        "      # This return sequence=True will help returning the sequences of the o/t per sample per time\n",
        "      # Also IF You Have  for example in our case , If we have very long sequences and we break  those sequences into smaller sentences and we want the state of different unit to be not reset , just state should be carried on\n",
        "      # We can set the stateful property as true to achive that\n",
        "\n",
        "\n",
        "      #// THEN WE HAVE OUR O?T LAYER\n",
        "      tf.keras.layers.Dense(VS)\n",
        "\n",
        "      # remember, we have 65 different character, so this dense layer must contain the number of units as unit for each characters\n",
        "\n",
        "  ])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KASMhlCS1Ktx"
      },
      "outputs": [],
      "source": [
        "#f_make_model()-- that is the fuction whichreturn the model, so lets call that model\n",
        "model=f_make_model(VS,ED,NU,batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tb27GKCU7V7B"
      },
      "source": [
        "*** ERROR WE GET***-\n",
        "---------------------------------------------------------------------------\n",
        "ValueError                                Traceback (most recent call last)\n",
        "/tmp/ipython-input-79-2551053607.py in <cell line: 0>()\n",
        "      1 #f_make_model()-- that is the fuction whichreturn the model, so lets call that model\n",
        "----> 2 model=f_make_model(VS,ED,NU,batch_size)\n",
        "\n",
        "2 frames\n",
        "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py in __init__(self, activity_regularizer, trainable, dtype, autocast, name, **kwargs)\n",
        "    285             self._input_shape_arg = input_shape_arg\n",
        "    286         if kwargs:\n",
        "--> 287             raise ValueError(\n",
        "    288                 \"Unrecognized keyword arguments \"\n",
        "    289                 f\"passed to {self.__class__.__name__}: {kwargs}\"\n",
        "\n",
        "ValueError: Unrecognized keyword arguments passed to Embedding: {'batch_input_shape': [50, None]}\n",
        "\n",
        "\n",
        "***WHY __üß† What This Error Means:\n",
        "The error tells us that the Embedding layer is not recognizing batch_input_shape as a valid argument.\n",
        "\n",
        "That‚Äôs because in recent versions of TensorFlow (2.14+), batch_input_shape is no longer directly accepted by the Embedding layer unless it's passed via an Input layer or a tf.keras.Input() tensor.\n",
        "‚úÖ How to Fix It:\n",
        "You have two correct options:\n",
        "\n",
        "‚úÖ Option 1: Use an Input layer with batch shape\n",
        "‚úÖ Option 2: Use input_length (only if stateful=False)\n",
        "If you don‚Äôt need stateful=True, you can drop batch_input_shape and just use:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pD4oOf4L8EAW"
      },
      "source": [
        "lets define the loss fuction :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkFRkMj-10hI"
      },
      "outputs": [],
      "source": [
        "def f_loss(y,y_hat):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(y,y_hat,from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaAmf9498kNb"
      },
      "outputs": [],
      "source": [
        "# Lets comple our model:\n",
        "model.compile(optimizer='adam',loss=f_loss)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6oFXOTr-FJO"
      },
      "source": [
        "*** IN THE NEXT BLOCK WE BASICALLY SET CHEAKPOINT WHICH ACTUALLY SAVE THE PARAMETERS OF THE MODEL WHICH WILL BE LATER USED IN THE TIME WHEN WE NEED TO GENERATION OF TEXT ***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1OY45Oq-zm-"
      },
      "source": [
        "*** IN THIS VIDEO OR BLOCK WE BASICALLY TRAIN THIS MODEL AND WE WILL ALSO SET DIFFERNENT CHEAKPOINTS , THERE ARE TWO WAYS OF  SAVING THE PARAMETTER OF THE MODEL\n",
        "* 1 IS THE YOU SAVE THE CHEAKPOINT\n",
        "* OTHER YOU SAVE THE MODEL ALONG WITH ITS ALL SPECIFICATION\n",
        "\n",
        "$ if yoU SAVE THE CHEAKPOINT THEN YOU NEED ,WHEN EVER YOU LOAD THE WEIGHT , YOU NEED THE DESCRIPTION OF THE MODEL IN THE SAME CODE FILE  LIKE HERE { Def f_make_model()}-- THAT IS THE DESCRIPTION OF THE MODEL AND OTHER SPECIFIC THAT DEFIND THE MODEL , YOU ONLY ACTUALLY SAVE THE WEIGHT RATHER THAN SPECIFICATIONS\n",
        "\n",
        "\n",
        "& ON THE OTHER HAND IF YOU SAVE THE MODEL USING THE SAVE MODEL THEN YOU DO NOT NEED DEFINE THE SPECIFIES  OF THE MODEL WHERE EVER YOU WANT IT USE IT , IT ACTUALLY CONTAIN ALL THE SPECIFICS AND THE SAVE MODEL IS NORMALLY USE AT THE DEVELOPMENT TIME  WHAN YOU REALLY WANT TO DEPLOY YOUR PRODUCT ***\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQGLUADR8w-G"
      },
      "outputs": [],
      "source": [
        "# Here WE Basically run the model(train our model) and we also set the different cheak point\n",
        "# there are Two ways saving Parametter of the model  or model itself:\n",
        "# 1--- is you save the cheak point and other ways is you save the whole model along with it specification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uj6ExeliBerI"
      },
      "source": [
        "*** IN THIS PARTICULAR VIDEO , WE ARE GOING TO USE CHEAKPIONT BEACUSE THIS IS THE FILE WHERE ALL THE SPECIFICS ARE DEFINED , SO JUST SAVE THE WEIGHT THAT CAN BE  LATER LOADED OR USE AT THE TEST TIME***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9cGVA5iBX8K"
      },
      "outputs": [],
      "source": [
        "# LETS FIRST DEFINE THE CHEAKPIONT DIRECTORY\n",
        "# sO IN LAYMEN TERM YOU CAN THINK OF AS THESE CHEAKPIONT SAY JUST SAVING THE MODEL , AT THE DIFFERENT ITERATION ANFD THE DIFFERENT EPOCHS YOU SAVE THE DIFFERNT CHEAKPIONT\n",
        "# ONCE THE MODEL IS TRAIN PROPERLY YOU PICK THE LATEST WEIGHT OR THE LATEST CHEAKPOINTS\n",
        "cheakpionts_dir='./tr_cheakpoints' # tHAT THE DIRECTORY WE REALLY WANT TO CREATE\n",
        "\n",
        "cheakpiont_prefix=os.path.join(cheakpionts_dir, \"cheak_{epoch}.weights.h5\")\n",
        "\n",
        "# cheak_{epoch} WE BASICALLY SET THE CHEAK PIONT FOR THR DIFFERNT EPOCHS , SO FOR EACH EPOCH WE SAVE THE PIONTS\n",
        "\n",
        "cheakpiont_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    # here we actually set the file paths which is cheakpiont prefix in our case\n",
        "    filepath=cheakpiont_prefix,\n",
        "    save_weights_only=True\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sk8_bDIlFzbZ"
      },
      "source": [
        "*** ERROR ---\n",
        "ValueError: When using `save_weights_only=True` in `ModelCheckpoint`, the filepath provided must end in `.weights.h5` (Keras weights format). Received: filepath=./tr_cheakpoints/cheak_{epoch}\n",
        "\n",
        "\n",
        "‚úÖ Explanation:\n",
        "When you set save_weights_only=True, TensorFlow expects the filename to end with:\n",
        "\n",
        "plaintext\n",
        "Copy\n",
        "Edit\n",
        ".weights.h5\n",
        "This is the standard Keras format for saved model weights.\n",
        "\n",
        "But your current filename is:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "'./tr_cheakpoints/cheak_{epoch}'\n",
        "It doesn‚Äôt include .weights.h5, so TensorFlow throws an error.\n",
        "\n",
        "‚úÖ Fix:\n",
        "Change:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "cheakpiont_prefix = os.path.join(cheakpionts_dir, \"cheak_{epoch}\")\n",
        "To:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "cheakpiont_prefix = os.path.join(cheakpionts_dir, \"cheak_{epoch}.weights.h5\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VHPGmsyHKvA"
      },
      "source": [
        "##################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n82I7L94HQOP"
      },
      "source": [
        "üîç What does it do?\n",
        "This line trains your model using the dataset you prepared, for 15 full passes (epochs) over the data, and saves model weights after each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFLKNAejEIAR",
        "outputId": "57d41ee8-b8a1-45da-bbc8-cf5c4d7a9baf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 54ms/step - loss: 3.1717\n",
            "Epoch 2/15\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 55ms/step - loss: 2.0309\n",
            "Epoch 3/15\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 57ms/step - loss: 1.7520\n",
            "Epoch 4/15\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 57ms/step - loss: 1.5794\n",
            "Epoch 5/15\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 58ms/step - loss: 1.4708\n",
            "Epoch 6/15\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 60ms/step - loss: 1.4035\n",
            "Epoch 7/15\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 62ms/step - loss: 1.3523\n",
            "Epoch 8/15\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 59ms/step - loss: 1.3105\n",
            "Epoch 9/15\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 61ms/step - loss: 1.2716\n",
            "Epoch 10/15\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 60ms/step - loss: 1.2359\n",
            "Epoch 11/15\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 60ms/step - loss: 1.2035\n",
            "Epoch 12/15\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - loss: 1.1727\n",
            "Epoch 13/15\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - loss: 1.1371\n",
            "Epoch 14/15\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 58ms/step - loss: 1.1029\n",
            "Epoch 15/15\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 59ms/step - loss: 1.0686\n"
          ]
        }
      ],
      "source": [
        "# NOW WE ARE REALLY READY TO TRAIN OUR MODEL\n",
        "\n",
        "histroy=model.fit(dataset,epochs=15,callbacks=[cheakpiont_callback])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yPeHmhVKMx5"
      },
      "source": [
        " # *** NEXT TEXT GENERATION PHASE***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GMO1gx1hHTfU"
      },
      "outputs": [],
      "source": [
        "# LETS WRITE  FUCTION TO GENERATE THE TEXT , BUT BEFORE GENERATING THE TEXT LETS BULID THE MODEL AGAIN WITH BATCH SIZE IS EQAULS TO 1\n",
        "# bECAUSE THAT IS REALLY SIMPLE TO BULID\n",
        "\n",
        "model=f_make_model(VS,ED,NU,1)# AS YOU SEE THE EARLYER WE HAVE THE BATCH SIZE IS 1000 BUT HERE WE CREATE THE AGIAN MODEL FOR SIMPLICITY AS\n",
        "# WE SEE THAT WE GIVE THE BATCH SIZE AS 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7e6CEjaPVBQ"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "latest_ckpt = sorted(glob.glob('./tr_cheakpoints/*.weights.h5'))[-1]\n",
        "model.load_weights(latest_ckpt)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " AFTER THAT LETS LOAD THE WEIGHT  FOR THE LATEST CHEAKPIONT IN THIS PARTICULAR MODEL\n",
        " SO model.load_weights ( From where we load the weights {from the latest cheakpiont(here we give the cheakpoint directory)})\n",
        "model.load_weights(tf.train.latest_checkpoint(cheakpionts_dir))"
      ],
      "metadata": {
        "id": "BQ7ws0qLQ9a6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "H6ptNvXHMxNi",
        "outputId": "0e3e0747-69fa-488c-c1f3-a25b1cb86bfe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./tr_cheakpoints'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "# IF  JUST PRINT THR CHEAKPOINT DIRECTORY\n",
        "cheakpionts_dir# That Basiaclly this particular directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsyT-G7oNRL8"
      },
      "outputs": [],
      "source": [
        "# So AFTER THAT LETS BUILD THE MODEL USING THE BATCH SIZE EQUALL TO ONE\n",
        "model.build(tf.TensorShape([1,None])) # SO YO SEE THE THE ANALOGY HERE WE SET batch_size IS EQUAL TO ONE  {{ EARLIER WHEN WE TRAIN WE SET THE BATCH SIZE[[ batch_input_shape[batch_szie,None]]]}}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*** LETS NOW WRITE A HELPER FUCTION THAT ACTUALLY GENERATE THE TEXT***"
      ],
      "metadata": {
        "id": "4F0gwRB8TOvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ss is start string(inital string)\n",
        "def f_write_now(model,ss):\n",
        "  # First Of all we defined the number of character we want to generate\n",
        "  N=10000 # Like WE generate the 100000 characters\n",
        "\n",
        "  # lets now actually write the C2I{character to index} , Lets Convert the intial string to their integer values\n",
        "  input_eval=[c2i[a] for a in ss]# this command will convert all the character string to integer indices, which the embedding layer will convert to dense vectors\n",
        "\n",
        "  # now we have to expand the dimension of this tensor shape\n",
        "  # so lets expand the dimension\n",
        "  input_eval=tf.expand_dims(input_eval,0)# what ever the dimenstion  are just expanded to meet the shape that is required\n",
        "\n",
        "\n",
        "  generated_text=[]# so o/t or generated text , o/t is gathered character by character as we predict\n",
        "  # then we set the model state to reset\n",
        "  model.layers[1].reset_states() # Access the GRU layer (index 1 in the Sequential model) and reset its states\n",
        "\n",
        "  # Now we write a loop for the total number of character that we really want to generate, and we generate the character one by one  and feed that generated character as input as to next rnn model as input\n",
        "  for i in range(N):\n",
        "    # First we make the prediction(p)\n",
        "    p=model(input_eval)\n",
        "    # lets Now convert the prediction to squeeze their dimension\n",
        "    p=tf.squeeze(p,0)# That is actually squeeze the dimension\n",
        "    # then we actually find out the most likely prediction using the categoriacl distribution , then we convert in numpy\n",
        "    p_id=tf.random.categorical(p,num_samples=1)[-1,0].numpy()\n",
        "\n",
        "    # then we find out input_eval again\n",
        "    input_eval=tf.expand_dims([p_id],0) # here we expand the dimenstion base on predition id and then we set that to zero\n",
        "    # Now our generated text is this particular prediction id{p_id}\n",
        "\n",
        "    # {  input_eval=tf.expand_dims([p_id],0)}--- this particular command is doing is basically is doing is whatever the predicted character, that is basically taking that also as the part of the input\n",
        "\n",
        "    generated_text.append(i2c[p_id])# we appened the generated text\n",
        "  return(ss + ''.join(generated_text))\n",
        "\n",
        "\n",
        "  # waht we return ( {whatever the start string  and we concated that {whatever the text we generated }})"
      ],
      "metadata": {
        "id": "aAKkeq9wTNb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now LETS CALL THIS FUNTION BY GIVING SOME INTIAL STRING\n",
        "print(f_write_now(model,ss=u\"india is the country: \"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xH1Q1lxaY9l",
        "outputId": "7f4364ad-1219-424f-df6b-fb1a326da4b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "india is the country: these\n",
            "deny his facour tire we have offer hour\n",
            "As it not had a vesome see me.\n",
            "\n",
            "LUCENTIO:\n",
            "Weremonio, good Harrant, I know thee. if thou otherds for his\n",
            "way, and as a thrifting-bear that a right.\n",
            "This fast, tilket when then, to piece\n",
            "A what which you may night'st tare to all assured come.\n",
            "Twite precious braining I am come with you\n",
            "Stand by a merolation, and most rule husband.\n",
            "\n",
            "CARILANA:\n",
            "I could not naved he had said speed.\n",
            "\n",
            "Canother!\n",
            "\n",
            "MERCUTIO:\n",
            "I am craved with mile: I, an adain, profordion;\n",
            "For whose when I may beg;' sed a very strange; For the door I\n",
            "speak of all.\n",
            "\n",
            "HERMIONE:\n",
            "I take on war.\n",
            "\n",
            "MARIANA:\n",
            "O hat i' the merrolmbe another\n",
            "we hadge, that Parus.\n",
            "\n",
            "QUEEN MARGARET:\n",
            "Off proples of Callariame: give twith tears,\n",
            "To give a sof my husband. Come you have suffer;\n",
            "I will not have lent this spare me here. There is, tongifle and not.\n",
            "\n",
            "TRANIO:\n",
            "Hark, Warwick, sir, shall hid clear you to his treasure\n",
            "I never marry me to-a mine; or a nation\n",
            "are widow the temples of thy halb.\n",
            "\n",
            "LUCENTIO:\n",
            "'TRANORUS:\n",
            "Then lies for even by her: good my heat of gald--moarnies: Hereford\n",
            "al keep in Richard and gelet please you, with a sail, sir; give obster,--\n",
            "you make you perfect all. Gentle me,\n",
            "And thre slougs of it for a while.\n",
            "\n",
            "ANTONIO:\n",
            "I doubt myself, bid him after rated.\n",
            "\n",
            "Gravondma: how she say; he would not make me pure,\n",
            "I must thus more general for thy seat is fairer\n",
            "To break by the stech cries out on my head.\n",
            "Gentleman me ondent?\n",
            "\n",
            "VONUS:\n",
            "Do you get of dims?\n",
            "\n",
            "GLOUCESTER:\n",
            "Hearies 'Eld not remember, and old lade your own\n",
            "To hinderly.\n",
            "\n",
            "SOPER:\n",
            "Arthoughnither,\n",
            "Go take; anon, go firra: forward, I must gate\n",
            "And go eneme: sin, I cannot be spite.\n",
            "\n",
            "ALONSO:\n",
            "Let's sake. Go, give me thine other both\n",
            "Dies against him, was a minthem word.\n",
            "\n",
            "HORTENSIO:\n",
            "Flist all my wife; and I a water of it and speed.\n",
            "\n",
            "PETRUCHIO:\n",
            "This earth,\n",
            "Now forsweet Bilaber, not a word, you throne joy's cock\n",
            "By phoppereaged and unwike owel. Come, you must,\n",
            "That thoughts that ear your tanks; I pray you.\n",
            "\n",
            "HORTENSIO:\n",
            "Clarence heavily one of an oul deliver'd.\n",
            "\n",
            "KING EDWARD IV:\n",
            "Was left me enough, im not be deserved.\n",
            "Pearn, mild, will be ready, her obeouth arms\n",
            "So tay, if so keep agua.\n",
            "\n",
            "GEORDEN:\n",
            "On Lady? and I all in no weak.\n",
            "\n",
            "Grovises! Nover Launte?\n",
            "\n",
            "LUCIO:\n",
            "'Till please yourself sake an old matter: go that mine\n",
            "Of spun't, and hearing of his simple garmen,\n",
            "Or else were, forrow at Laus of kind\n",
            "Of revore so faire honour, Tybalt, slangerous life, forget with spack,\n",
            "That cheer will have it done. Pend about\n",
            "my hand, and I forgot it is to the voice:\n",
            "The war guttle-governed this weak of Me, thou delitatest!\n",
            "We will tell thy name a' my wife. Is't\n",
            "Are they are cosible.\n",
            "\n",
            "MIRANDA:\n",
            "Pardon, friar, amisious makes the curning of her wit!\n",
            "Comfort were not with me.\n",
            "\n",
            "PETRUCHIO:\n",
            "Procord and lacks. But have I sto\n",
            "winder by her bright wantons by that younger stone:\n",
            "Go with my rare be counterpance.\n",
            "\n",
            "BIONDELLO:\n",
            "Good and air, three precentious son,\n",
            "Though they do and scare, my mouth hath that a\n",
            "copple-wood fallion, by those head'st three is.\n",
            "\n",
            "TRANIO:\n",
            "A mother that come upon my nest to seem\n",
            "Before my citizens of Barnardone?\n",
            "'Tis more t not of her governne already; 'tis by neighbour it not,\n",
            "And not I have been speakell thus.\n",
            "\n",
            "FLORIZEL:\n",
            "Ay, sir, to you may please? heas the war,\n",
            "I'll have thee speak all know:\n",
            "I will beseech a sick man, that fear thence, and to say I stand go expect thee,\n",
            "Hath closeled all your under my life,\n",
            "Like a bred virtue: therefore at knew not cheast.\n",
            "Fier and am zenedative.\n",
            "\n",
            "VINCENTIO:\n",
            "Will't please? they kiss the command, and 'twas pated on\n",
            "a feigh, for a chargh of heaven hither graw.\n",
            "\n",
            "GONZALO:\n",
            "I am to his envy ring.\n",
            "\n",
            "ANTONIO:\n",
            "Wedgaids,\n",
            "And I am an ambition.\n",
            "Thou hot too great a golden poor in crimior\n",
            "of Barnardine seth call: I have\n",
            "noce a traitor too.\n",
            "\n",
            "BONTONYO:\n",
            "Tell that folloys too truep. I would thou may\n",
            "belike me here and reverand thee it catch\n",
            "For mine honour: I lay not doubt? 'Ty mine; st dish's mine,\n",
            "And where feer this bloody man Divil, I\n",
            "Side!\n",
            "Be cust, and be a brooth for your woe:\n",
            "Is thile you fare? that's is this gone?\n",
            "\n",
            "BEONT:\n",
            "I'll see thee. Finguish, be ready, by live her, he have man denies to beaute hither.\n",
            "\n",
            "LEONTES:\n",
            "Ed, but I saw myself at end\n",
            "Tedmit his painting by this daughter and bask for,\n",
            "And the town mother with choos that call'd my brey-dlow his happiers;\n",
            "And hast his thing schoolmaster too shorth\n",
            "As nine shall make thee chusches; brunk the present seas,\n",
            "There wanton mean can blush swear than the proud widow?\n",
            "\n",
            "CAPILLO:\n",
            "Why, go, and what valiant domemain;\n",
            "And speak no more.\n",
            "\n",
            "HORTENSIO:\n",
            "\n",
            "TillWhen then they had winn;\n",
            "For night may chief.\n",
            "\n",
            "DUCHESS OF YORK:\n",
            "Cution God's hard, my Lord:\n",
            "All I kind, to live a day and crowble shull fire.\n",
            "\n",
            "PETRUCHIO:\n",
            "We'll have good steed King Lebutable:\n",
            "My favour hearts and sprife: have mounded to look a.\n",
            "I must high power: bleave you, sir: it is to-day,\n",
            "When I wish my prayers; that I should be yours cass, is right.\n",
            "\n",
            "LUCENTIO:\n",
            "A very cause and faurth to such. Could not?\n",
            "\n",
            "Servant:\n",
            "Mastakneraty, Patialenge!\n",
            "\n",
            "BAPTISTA:\n",
            "He deserves his brows.\n",
            "\n",
            "GREMIO:\n",
            "You art we came, and by thif on my graven.\n",
            "\n",
            "ANTONIO:\n",
            "The mile, since we do.\n",
            "\n",
            "ANTONIO:\n",
            "Truere.\n",
            "\n",
            "GRUMIO:\n",
            "'Tis but by live her.\n",
            "\n",
            "GREMIO:\n",
            "Ay, four thinkering of thy sons find\n",
            "thouar itself have either laughter show\n",
            "The faults, one seo.\n",
            "It is not he is closed to kiss; but you might hear:\n",
            "Which, sir, and frear is to the cruppraooth?\n",
            "\n",
            "VOLUMNIA:\n",
            "What, tait, it go to? warrait,\n",
            "He should he cried it.\n",
            "\n",
            "DORCAS:\n",
            "Well, like your cheeds.\n",
            "\n",
            "GEORTES:\n",
            "Your bid I grow-pitided igle.\n",
            "\n",
            "PETRUCHIO:\n",
            "O sir, I do we attend and good priered,\n",
            "Spooking a grimisity travell togothereis house,\n",
            "If we with you, gin'd me nie.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Wence thy father father of guilt!\n",
            "\n",
            "ANGELO:\n",
            "\n",
            "BIONDELLO:\n",
            "They more than my hands! A flouers are looking over forsen Buhavio!\n",
            "Warrant me and sent else hear me, not the dock so nate\n",
            "That did knock buczed air golden death,\n",
            "If you most servant:\n",
            "If to him, 'pabated jon, and thrice-hooud\n",
            "to heaven have pernoced, duty to Petect,\n",
            "Doous pethat three llords here tree. Nays thou havest duke's oced-moul\n",
            "I must bear a broken rage was scalved's that; it maken her;\n",
            "'Tis very with me.\n",
            "\n",
            "PETRUCHIO:\n",
            "O, thou dast duty, and we'll ply informit either\n",
            "ritoly.\n",
            "\n",
            "TRANIO:\n",
            "His smring is so; yet I may call thee well.\n",
            "All hath mp not yet give you thee and will I like to you;\n",
            "Who would tell him with all things send forth ere\n",
            "He could from her for the contractions for a ser\n",
            "applitted than this doinger.\n",
            "\n",
            "ANTONIO:\n",
            "I have discered,\n",
            "Mark a perry. It is the wiser sort\n",
            "Bohemia this the purpose in the sun,\n",
            "I can make weep,\n",
            "Being with rison, on the justice off\n",
            "To knock belongeding a friend, and a widow\n",
            "Of kind of thither,\n",
            "By supher and a kins\n",
            "Five cribanio's susperage, lay one gold\n",
            "might have your treason was with'd The\n",
            "Answer hero, to Bianish cries alone,--\n",
            "\n",
            "CORIOLANUS:\n",
            "Yes; one Kath; but gone for Henry's followers:\n",
            "He makedess love complitity counsel is himself,\n",
            "Twan their leaveness that doth think the people,\n",
            "The devil I not then attend me, not\n",
            "In these wench s it\n",
            "parted out to latie. Fare you bid him by time ford ten;\n",
            "Take I should spoken it siting, in\n",
            "your delight, decyes, and so it was all.\n",
            "\n",
            "ANTONIO:\n",
            "My play thus farmello, go you not.\n",
            "Good my son, I, will you milline oy it, Mysullen:\n",
            "I'll take again to-niges but terms and spool I not show milling to bristed away thes,\n",
            "And that say you have capted oney fa?\n",
            "\n",
            "MistoNGO:\n",
            "A thought of her!\n",
            "\n",
            "HASTINGS:\n",
            "To thy confender: I let you stry welcome, wicked\n",
            "By too.\n",
            "\n",
            "ANTONIO:\n",
            "All the party of our reformord,\n",
            "What is the walm-mak in this rark-mouth and by Scock-and left, have tated my death?\n",
            "\n",
            "SAMPENS Merelove.\n",
            "\n",
            "PETRUCHIO:\n",
            "Dight drop you should so.\n",
            "\n",
            "BIANCA:\n",
            "Welcome edom,--\n",
            "As God and Bapuisfult! The usurel is empty\n",
            "As marry me to action: else but seek'd\n",
            "And did before me; but it is it not\n",
            "ande more from him, and husband.\n",
            "\n",
            "Lord:\n",
            "Master, my care on't.\n",
            "What, hunt? Pompey? you that be done.\n",
            "\n",
            "Lurelly:\n",
            "Ha! Wedn, make wander that doth mine alseech for pruse\n",
            "Of water that will not git thyself forery:\n",
            "The very musicquit the ear-feat been slain;\n",
            "And if so she sun and all expossly.\n",
            "\n",
            "ANGELO:\n",
            "I prithee, you were be, for we hear! let them\n",
            "that hasd mend here befole thy beauty,\n",
            "Signior Begon a crow-kindly;\n",
            "now a passing gods; which 's to the world.\n",
            "\n",
            "CAMILLO:\n",
            "I that new you walk again and firm; boy,\n",
            "To pray your father catche our heav; unwere my wooonds.\n",
            "\n",
            "PETRUCHIO:\n",
            "Were thou the mean to say, Depused firewnd.\n",
            "Or should slew him a dream kindnow close our heart\n",
            "Talk worred. Ney have you bot off thee in thee;\n",
            "Thus tomanks with my face, if I\n",
            "came find on fuedly, orce.\n",
            "\n",
            "AUTOLYCUS:\n",
            "I hone,\n",
            "When thral Ise teepait, sir, yef of women's throne.\n",
            "\n",
            "GREMIO:\n",
            "I whom such ages so gird.\n",
            "\n",
            "GLOUCESTER:\n",
            "What, tooking the ishut dow?\n",
            "\n",
            "GLOUCESTER:\n",
            "That I have founday Myst terrore of Berkety, I swore, is fairly\n",
            "Amparried; therefore should but wine. Welcome, brave in closed in these\n",
            "Villuner, and disposed thence and thing\n",
            "That lies young and scenterback of the blood\n",
            "In that my noble grace miser'd but something a gently.\n",
            "\n",
            "ANGELO:\n",
            "In the marrith cap deliver a tard for mine.\n",
            "Will'e KANGEL:\n",
            "There obey bid me Along.\n",
            "\n",
            "CAPULET:\n",
            "The inly use your huther for beasts was after honest\n",
            "Teble or busine, the valient till I sea My cold;\n",
            "And was as baw promised by their both.\n",
            "\n",
            "STANLEY:\n",
            "In Gentle cit is the church, truth on fear-nell-home to deep thee, is nee 'he\n",
            "A plead-factors. But thy place is not at home:\n",
            "Then lia: within husband may he would condenting\n",
            "Apart within, Signior Baptista,, good Paulina! Sir Jeamoward.\n",
            "\n",
            "KATHARINA:\n",
            "You mean my breath I a shepherd recorded by,\n",
            "And I be remed; with all liken ear hold of my heart.\n",
            "\n",
            "PETRUCHIO:\n",
            "O sir, it is it soundly by dreads, to the year,\n",
            "I mean thee in his man. Believe him; for wine!\n",
            "A pleasant will murder!\n",
            "\n",
            "Servant:\n",
            "Unto the perisan, pubble means my life: it may, bold,\n",
            "Citizen! a done? for when I be young\n",
            "And to deligh to have an under end to die?\n",
            "\n",
            "ANTONIO:\n",
            "The command of rich. Come, I have been\n",
            "the head blessed labours were toother of the people:\n",
            "But, we gown, what to compority, he reterds\n",
            "Unless upon their air co\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W-KzalLJayY2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyOUgje17uVX/XeEy473LdiW",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}