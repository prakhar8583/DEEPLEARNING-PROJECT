{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN80HfoLq1IEQYZRrRk5Z3f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prakhar8583/DEEPLEARNING-PROJECT/blob/main/TEXTCLASSIFICATION_USING_RNN_PROJECT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKIJ01VjuO2m"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NEXT We IMPORT THE THE SOME DATA SET FROM THE TENSORFLOW DATA SETS , as well information about the data sets , this inforation actually encoder or embedder as well , that encode or embedding the the eveey word to some integer\n",
        "# // BY{auther}-dataset, info=tfd.load('imdb_reviews/subwords8k',with_info=True,as_supervised=True)\n",
        "# NEXT We IMPORT THE THE SOME DATA SET FROM THE TENSORFLOW DATA SETS , as well information about the data sets , this inforation actually encoder or embedder as well , that encode or embedding the the eveey word to some integer\n",
        "dataset, info = tfds.load('imdb_reviews', with_info=True, as_supervised=True)"
      ],
      "metadata": {
        "id": "Ig1FZD7hudXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets Now the pick the train data{tD} sets as well as the testing data set or the validation data{vD} set\n",
        "tD,vD=dataset['train'],dataset['test']"
      ],
      "metadata": {
        "id": "7hHlakr6u_Cf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then build your subword encoder manually:"
      ],
      "metadata": {
        "id": "CS9gRSwnv9ac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This creates a tokenizer that breaks sentences into subwords (like \"mov\", \"ing\", \"bad\", \"ly\"). The build_from_corpus() method analyzes all text and builds a vocabulary of up to 8000 subwords.\n",
        "\n",
        "🧠 Example:\n",
        "Sentence: \"This movie was terrible!\"\n",
        "Subword encoding: [15, 232, 9, 1794, 87]"
      ],
      "metadata": {
        "id": "6eP0jBuN9iLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    (text.numpy() for text, _ in tD), target_vocab_size=8000)\n"
      ],
      "metadata": {
        "id": "V2ylazvwvCdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let We Write some strings :\n",
        "s='Hello,how are you I am  prakhar Singh on this time I am thing I Creting some world changing project but If in future I saw this I will laught too much I do not wheter my feture will go but I belive I am At a good place  and all the best for the next upcomming life by'\n",
        "# lets encode the string into sequence of intergers\n",
        "# es-- encoder string It Is basically have the index of the each word from the strings\n",
        "es= encoder.encode(s)# es is the encoder strings"
      ],
      "metadata": {
        "id": "fTpI1W7hvGWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "es# now You have this (es) is encoded strings you can see this"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2zRjshnvlUx",
        "outputId": "593c720d-b993-42d3-dda4-a70fb7fbacfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4093,\n",
              " 7914,\n",
              " 7847,\n",
              " 113,\n",
              " 29,\n",
              " 37,\n",
              " 12,\n",
              " 888,\n",
              " 7835,\n",
              " 7835,\n",
              " 2791,\n",
              " 3437,\n",
              " 1504,\n",
              " 3887,\n",
              " 6737,\n",
              " 25,\n",
              " 14,\n",
              " 102,\n",
              " 12,\n",
              " 257,\n",
              " 232,\n",
              " 12,\n",
              " 2612,\n",
              " 443,\n",
              " 63,\n",
              " 382,\n",
              " 7774,\n",
              " 7835,\n",
              " 4838,\n",
              " 26,\n",
              " 156,\n",
              " 11,\n",
              " 1875,\n",
              " 12,\n",
              " 283,\n",
              " 14,\n",
              " 12,\n",
              " 96,\n",
              " 1292,\n",
              " 21,\n",
              " 150,\n",
              " 97,\n",
              " 12,\n",
              " 112,\n",
              " 33,\n",
              " 4024,\n",
              " 903,\n",
              " 82,\n",
              " 4768,\n",
              " 1304,\n",
              " 96,\n",
              " 187,\n",
              " 26,\n",
              " 12,\n",
              " 3076,\n",
              " 460,\n",
              " 12,\n",
              " 257,\n",
              " 501,\n",
              " 4,\n",
              " 74,\n",
              " 894,\n",
              " 7835,\n",
              " 7835,\n",
              " 5,\n",
              " 51,\n",
              " 1,\n",
              " 175,\n",
              " 23,\n",
              " 1,\n",
              " 633,\n",
              " 304,\n",
              " 3294,\n",
              " 35,\n",
              " 220,\n",
              " 1140]"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# you can actually the vocabulary size of this encoder as well\n",
        "encoder.vocab_size # here The vocabularyulary size the enocoder is using is 8059 words in vocabulary , you can also for example see which index belongs to which word in the voca"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-7_U2vFv7Qe",
        "outputId": "06f9057f-a29e-43c4-e6bd-646f53a9b11f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8059"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You Can For Example see which index belongs to which word in the vocabulary"
      ],
      "metadata": {
        "id": "XK0a2arfx8tu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for idx in es:\n",
        "  print('{}-->{}'.format(idx,encoder.decode([idx])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zU2WBJwJwx7w",
        "outputId": "896e661f-c19f-4167-d8d3-d334b1b11a76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4093-->Hell\n",
            "7914-->o\n",
            "7847-->,\n",
            "113-->how \n",
            "29-->are \n",
            "37-->you \n",
            "12-->I \n",
            "888-->am\n",
            "7835--> \n",
            "7835--> \n",
            "2791-->pra\n",
            "3437-->kh\n",
            "1504-->ar \n",
            "3887-->Sin\n",
            "6737-->gh \n",
            "25-->on \n",
            "14-->this \n",
            "102-->time \n",
            "12-->I \n",
            "257-->am \n",
            "232-->thing \n",
            "12-->I \n",
            "2612-->Cre\n",
            "443-->ting \n",
            "63-->some \n",
            "382-->world \n",
            "7774-->changing\n",
            "7835--> \n",
            "4838-->project \n",
            "26-->but \n",
            "156-->If \n",
            "11-->in \n",
            "1875-->future \n",
            "12-->I \n",
            "283-->saw \n",
            "14-->this \n",
            "12-->I \n",
            "96-->will \n",
            "1292-->laugh\n",
            "21-->t \n",
            "150-->too \n",
            "97-->much \n",
            "12-->I \n",
            "112-->do \n",
            "33-->not \n",
            "4024-->whe\n",
            "903-->ter \n",
            "82-->my \n",
            "4768-->fet\n",
            "1304-->ure \n",
            "96-->will \n",
            "187-->go \n",
            "26-->but \n",
            "12-->I \n",
            "3076-->bel\n",
            "460-->ive \n",
            "12-->I \n",
            "257-->am \n",
            "501-->At \n",
            "4-->a \n",
            "74-->good \n",
            "894-->place\n",
            "7835--> \n",
            "7835--> \n",
            "5-->and \n",
            "51-->all \n",
            "1-->the \n",
            "175-->best \n",
            "23-->for \n",
            "1-->the \n",
            "633-->next \n",
            "304-->up\n",
            "3294-->comm\n",
            "35-->ing \n",
            "220-->life \n",
            "1140-->by\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.encode('152')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Q5g0v3gxzVg",
        "outputId": "be67b913-523c-4e78-a46a-aa05ee3d64ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[7852, 7856, 7853]"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After rhat we have our encoder is availble Lets Go to  and suffle our training data  or make A batach size of our training data"
      ],
      "metadata": {
        "id": "ElacQkwEhErL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We shuffle the training data to ensure that the model doesn't learn patterns based on the order of the data, which can lead to overfitting or poor generalization.\n",
        "\n",
        "Here's why shuffling is important:\n",
        "1. Avoid learning order-based bias\n",
        "If your training dataset is ordered (e.g., all examples of class A come first, then class B), the model might pick up on these artificial patterns instead of learning the actual features of each class.\n",
        "\n",
        "e Shuffling helps expose the model to a more diverse set of examples in each batch, making the training process more robust and better at generalizing to unseen data.\n",
        "3. Works better with batch training\n",
        "When using batch training (e.g., padded_batch(50)), each batch should ideally represent the overall data distribution. If the data is not shuffled, the early batches may be biased, which slows down convergence or leads to instability.\n",
        "\n",
        "3. Works better with batch training\n",
        "When using batch training (e.g., padded_batch(50)), each batch should ideally represent the overall data distribution. If the data is not shuffled, the early batches may be biased, which slows down convergence or leads to instability.\n",
        "\n",
        "In the context of a movie review dataset (such as IMDb reviews labeled as positive or negative), learning order-based bias refers to the model accidentally learning patterns based on the order of the data — not on the content of the reviews themselves.\n",
        "\n",
        "🔎 Example Scenario:\n",
        "Imagine your dataset is ordered like this:\n",
        "\n",
        "First 25,000 reviews → all negative\n",
        "\n",
        "Next 25,000 reviews → all positive\n",
        "\n",
        "If you train the model on this unshuffled data, here's what might go wrong:\n",
        "\n",
        "⚠️ What the model might (wrongly) learn:\n",
        "In the early training steps, the model only sees negative reviews. It starts associating all reviews with negativity.\n",
        "\n",
        "Then suddenly it starts seeing only positive reviews — this shift can confuse the model or cause it to forget earlier patterns (a problem called catastrophic forgetting).\n",
        "\n",
        "If batches are built from sequential data, some batches will contain only negative or only positive reviews. These unbalanced batches make learning unstable.\n",
        "\n",
        "✅ When you shuffle:\n",
        "Each batch contains a mix of positive and negative reviews.\n",
        "\n",
        "The model learns to focus on the actual text features (like \"boring plot\", \"great acting\") rather than assuming a certain sentiment is more likely just based on batch order.\n",
        "\n",
        "This improves generalization and training stability.\n",
        "\n",
        "TL;DR:\n",
        "Learning order-based bias happens when the model picks up on artificial patterns from the sequence in which training data is presented, rather than learning meaningful patterns from the data itself. In movie reviews, this might mean wrongly associating early or late positions in the dataset with specific sentiments.\n",
        "\n",
        "Let me know if you'd like a diagram to visualize this or a code demo using IMDb dataset.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "what is tD.padded_batch\n",
        "\n",
        "\n",
        "ChatGPT said:\n",
        "tD.padded_batch() is a method from TensorFlow’s tf.data.Dataset API that batches elements of a dataset and pads them so all elements in a batch have the same shape. This is especially useful when working with variable-length sequences like sentences or time series data.\n",
        "\n",
        "Why use padded_batch()?\n",
        "In deep learning, especially in NLP or sequence tasks, input sequences (e.g., sentences) often vary in length. Neural networks need input tensors of the same shape per batch — that's where padding comes in.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZlYvHXtSkYsi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function takes raw text and label (e.g., (\"The movie was good\", 1)), and converts the text into a list of integers using the tokenizer (encoder.encode()).\n",
        "\n",
        "\n",
        "Input: \"I hated this film\"\n",
        "Output: [54, 302, 18, 91], Label: 0"
      ],
      "metadata": {
        "id": "CRn4lLOE91fP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TensorFlow datasets are graph-based and don’t run regular Python code easily.\n",
        "\n",
        "So we use tf.py_function to wrap our encode() logic.\n",
        "\n",
        "After that, we manually set shapes because TensorFlow loses shape info in this process."
      ],
      "metadata": {
        "id": "xbdINqeT-DRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode function for mapping\n",
        "def encode(text, label):\n",
        "    encoded_text = encoder.encode(text.numpy())\n",
        "    return encoded_text, label\n",
        "\n",
        "# Wrap in tf.py_function to use with tf.data\n",
        "def encode_map_fn(text, label):\n",
        "    encoded_text, label = tf.py_function(encode, inp=[text, label], Tout=(tf.int64, tf.int64))\n",
        "    encoded_text.set_shape([None])  # shape must be specified after tf.py_function\n",
        "    label.set_shape([])             # scalar label\n",
        "    return encoded_text, label\n",
        "\n",
        "#// This step applies the encoder function to every (text, label) pair in training and validation sets.\n",
        "\n",
        "# Apply encoding\n",
        "tD = tD.map(encode_map_fn)\n",
        "vD = vD.map(encode_map_fn)\n"
      ],
      "metadata": {
        "id": "hHQLL7Xe2NOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle, pad, and batch\n",
        "tD = tD.shuffle(1000).padded_batch(50)\n",
        "vD = vD.padded_batch(50)"
      ],
      "metadata": {
        "id": "aFf-BQNIyydI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  LETS BULID OUR RNN MODELS"
      ],
      "metadata": {
        "id": "6pmM8cFViISi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔷 What is an Embedding Layer?\n",
        "The embedding layer is the first layer in many text-processing models (like for movie reviews). Its job is to take integer-encoded words (like 4, 53, 912) and convert them into dense, continuous vectors (like [0.12, -0.45, 0.78, ...]).\n",
        "\n",
        "🔹 Why do we need embeddings?\n",
        "Raw words (or tokens) are first converted to integers (via a vocabulary). But neural networks can’t learn from integers directly — they need numerical features that capture meaning. That’s what embeddings do.\n",
        "\n",
        "🔸 Dense Vector = Real-Valued Feature Vector\n",
        "A “dense vector” just means:\n",
        "\n",
        "It’s a fixed-length list of real numbers (like 50, 100, or 300 values).\n",
        "\n",
        "It captures semantic meaning: similar words will have similar embeddings."
      ],
      "metadata": {
        "id": "T0Xk9GZnmfx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    # First layer is the embedding layer ,the embedding layer required the total number of vocabulary and the output dimension\n",
        "    tf.keras.layers.Embedding(encoder.vocab_size, 10),\n",
        "    tf.keras.layers.LSTM(12),\n",
        "    # we adding the dense layer the o/t layer { the layer towards the o/t }, lets say this particular layer have just 5 units\n",
        "    tf.keras.layers.Dense(5),\n",
        "    # finally we have the o/t layer for softmax which have just 0ne unit, beacues it is binary classification problem\n",
        "\n",
        "    tf.keras.layers.Dense(1)\n",
        "])"
      ],
      "metadata": {
        "id": "8u-7cjjviFFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), optimizer=tf.keras.optimizers.Adam(1e-3), metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# This adam()--- default laerning rate or we acn specified the learning rate like adam(1e-3)"
      ],
      "metadata": {
        "id": "3GQTVzQGrNgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔍 Let’s break it down:\n",
        "model.fit(): Starts training the model using your training data (tD).\n",
        "\n",
        "epochs=2: The model will go through the entire training dataset 2 times.\n",
        "\n",
        "validation_data=vD: Tells the model to evaluate how well it performs on the validation dataset (vD) after each epoch.\n",
        "\n",
        "validation_steps=2: The model will use 2 batches from vD to compute validation loss/accuracy."
      ],
      "metadata": {
        "id": "S3YSZMpNs7au"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now model is ready lets fit the model\n",
        "history=model.fit(tD,epochs=2,validation_data=vD, validation_steps=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ojHgJoXscTD",
        "outputId": "90a3d1a2-981e-4563-a9e0-90ba2557ff00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m338s\u001b[0m 667ms/step - accuracy: 0.5004 - loss: 0.6933 - val_accuracy: 0.4993 - val_loss: 0.6933\n",
            "Epoch 2/2\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m408s\u001b[0m 719ms/step - accuracy: 0.5013 - loss: 0.6933 - val_accuracy: 0.4993 - val_loss: 0.6932\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# futher You have This validation loss amnd validation accuracy , using this model.evaluate , there you can give the validation data\n",
        "vL,vA=model.evaluate(vD)\n",
        "# that is actually computing the validation loss  and validation accuracy , by using the evalute methord\n",
        "# lets SEE It Is Using The same Model but that time it given the validation data to model to check its accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlCHiSbh08Qy",
        "outputId": "7524b88a-1a54-4d5e-fdbc-38368538caf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 189ms/step - accuracy: 0.5022 - loss: 0.6931\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vL,vA)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kehuPtd_Djp",
        "outputId": "4c25b23d-6c48-4f84-eba0-c1cc49f8b4b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6930656433105469 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VDeeZ9P5_o80"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}